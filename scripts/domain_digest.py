"""
åˆ†é ˜åŸŸæ¨æ’­ç³»çµ±

æ ¹æ“šä¸åŒé ˜åŸŸå¾ RSS ç²å–å…§å®¹ä¸¦æ¨æ’­åˆ° Telegram
æ”¯æ´çš„é ˜åŸŸï¼šé†«å­¸, AI, åœ‹éš›, GitHub, çŸ¥è­˜
"""
import sys
import argparse
import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional

# è¨­å®š stdout ç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

from config import (
    TELEGRAM_BOT_TOKEN,
    TELEGRAM_CHAT_ID,
    ANTHROPIC_API_KEY,
    validate_config
)

# é ˜åŸŸé…ç½®
DOMAIN_CONFIG = {
    "medical": {
        "name": "é†«å­¸",
        "emoji": "ğŸ¥",
        "feeds": [
            {"name": "PubMed (ECMO/VAD/Cardiac)", "url": "https://pubmed.ncbi.nlm.nih.gov/rss/search/1V_PRf-wigmmSOdeKS_0FLDNjB4gkI0R2Ppj3T0WTkRZDZugxK/?limit=15&utm_campaign=pubmed-2&fc=20250126040620"},
        ],
        "max_items": 10,
        "use_ai_filter": True,
        "default_hours": 48  # PubMed æ›´æ–°è¼ƒæ…¢ï¼Œä½¿ç”¨ 48 å°æ™‚
    },
    "ai": {
        "name": "AI",
        "emoji": "ğŸ¤–",
        "feeds": [
            # æ ¸å¿ƒ Blog
            {"name": "Simon Willison", "url": "https://simonwillison.net/atom/everything/"},
            {"name": "Anthropic", "url": "https://www.anthropic.com/rss.xml"},
            {"name": "OpenAI", "url": "https://openai.com/blog/rss.xml"},
            {"name": "Google AI", "url": "https://blog.google/technology/ai/rss/"},
            {"name": "Hugging Face", "url": "https://huggingface.co/blog/feed.xml"},
            # Newsletter
            {"name": "Latent Space", "url": "https://www.latent.space/feed"},
            {"name": "Import AI", "url": "https://importai.substack.com/feed"},
            {"name": "Ben's Bites", "url": "https://bensbites.beehiiv.com/feed"},
            # Reddit
            {"name": "r/MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/top/.rss?t=day"},
            {"name": "r/LocalLLaMA", "url": "https://www.reddit.com/r/LocalLLaMA/top/.rss?t=day"},
            {"name": "r/ClaudeAI", "url": "https://www.reddit.com/r/ClaudeAI/top/.rss?t=day"},
            {"name": "r/ChatGPT", "url": "https://www.reddit.com/r/ChatGPT/top/.rss?t=day"},
            {"name": "r/artificial", "url": "https://www.reddit.com/r/artificial/top/.rss?t=day"},
        ],
        "max_items": 10,
        "use_ai_filter": True
    },
    "international": {
        "name": "åœ‹éš›æƒ…å‹¢",
        "emoji": "ğŸŒ",
        "feeds": [
            # æ·±åº¦åˆ†æ
            {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml"},
            {"name": "Foreign Policy", "url": "https://foreignpolicy.com/feed/"},
            {"name": "Project Syndicate", "url": "https://www.project-syndicate.org/rss"},
            # å³æ™‚æ–°è
            {"name": "Reuters World", "url": "https://www.rss.reuters.com/news/worldnews"},
            {"name": "BBC World", "url": "https://feeds.bbci.co.uk/news/world/rss.xml"},
            {"name": "AP News", "url": "https://apnews.com/world-news.rss"},
            # Reddit
            {"name": "r/geopolitics", "url": "https://www.reddit.com/r/geopolitics/top/.rss?t=day"},
            {"name": "r/worldnews", "url": "https://www.reddit.com/r/worldnews/top/.rss?t=day"},
        ],
        "max_items": 10,
        "use_ai_filter": True
    },
    "github": {
        "name": "GitHub/é–‹ç™¼",
        "emoji": "ğŸ’»",
        "feeds": [
            {"name": "GitHub Trending (Python)", "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml"},
            {"name": "GitHub Trending (All)", "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml"},
            {"name": "Claude Code Releases", "url": "https://github.com/anthropics/claude-code/releases.atom"},
            # Reddit
            {"name": "r/programming", "url": "https://www.reddit.com/r/programming/top/.rss?t=day"},
            {"name": "r/webdev", "url": "https://www.reddit.com/r/webdev/top/.rss?t=day"},
            {"name": "r/Python", "url": "https://www.reddit.com/r/Python/top/.rss?t=day"},
        ],
        "max_items": 10,
        "use_ai_filter": True
    },
    "knowledge": {
        "name": "çŸ¥è­˜/ç”Ÿç”¢åŠ›",
        "emoji": "ğŸ“š",
        "feeds": [
            # ä¸­æ–‡
            {"name": "é›»è…¦ç©ç‰©", "url": "https://www.playpcesor.com/feeds/posts/default?alt=rss"},
            {"name": "å°‘æ•°æ´¾", "url": "https://sspai.com/feed"},
            {"name": "é–±è®€å‰å“¨ç«™", "url": "https://readingoutpost.com/feed/"},
            # è‹±æ–‡
            {"name": "Hacker News Best", "url": "https://hnrss.org/best"},
            {"name": "Farnam Street", "url": "https://fs.blog/feed/"},
            {"name": "Wait But Why", "url": "https://waitbutwhy.com/feed"},
        ],
        "max_items": 8,
        "use_ai_filter": True
    },
    "claude-code": {
        "name": "Claude Code æ›´æ–°",
        "emoji": "âš¡",
        "feeds": [
            {"name": "Claude Code Releases", "url": "https://github.com/anthropics/claude-code/releases.atom"},
        ],
        "max_items": 5,
        "use_ai_filter": True,  # ç”¢ç”Ÿç‰ˆæœ¬æ‘˜è¦
        "default_hours": 168  # ä¸€é€±å…§çš„æ›´æ–°
    }
}


def fetch_rss_feed(url: str, hours: int = 24) -> List[Dict]:
    """
    å¾ RSS feed ç²å–æœ€è¿‘çš„æ–‡ç« 

    Args:
        url: RSS feed URL
        hours: ç²å–éå»å¹¾å°æ™‚çš„æ–‡ç« 

    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    try:
        feed = feedparser.parse(url)
        articles = []
        cutoff = datetime.now() - timedelta(hours=hours)

        for entry in feed.entries[:20]:  # æœ€å¤šè™•ç† 20 ç¯‡
            # è§£æç™¼å¸ƒæ™‚é–“
            published = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6])

            # å¦‚æœç„¡æ³•è§£ææ™‚é–“æˆ–æ–‡ç« å¤ªèˆŠï¼Œè·³é
            if published and published < cutoff:
                continue

            articles.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", "")[:200] if entry.get("summary") else "",
                "published": published
            })

        return articles
    except Exception as e:
        print(f"  Error fetching {url}: {e}")
        return []


def fetch_domain_articles(domain: str, hours: int = 24) -> List[Dict]:
    """
    ç²å–ç‰¹å®šé ˜åŸŸçš„æ‰€æœ‰æ–‡ç« 

    Args:
        domain: é ˜åŸŸåç¨± (ai, international, github, knowledge)
        hours: ç²å–éå»å¹¾å°æ™‚çš„æ–‡ç« 

    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    config = DOMAIN_CONFIG.get(domain)
    if not config:
        print(f"Unknown domain: {domain}")
        return []

    all_articles = []

    for feed in config["feeds"]:
        print(f"  Fetching {feed['name']}...")
        articles = fetch_rss_feed(feed["url"], hours)
        for article in articles:
            article["source"] = feed["name"]
        all_articles.extend(articles)
        print(f"    Found {len(articles)} articles")

    return all_articles


# ç”¨æˆ¶åå¥½ Profileï¼ˆæ ¹æ“šé–±è®€æ­·å²å’Œç­†è¨˜åº«åˆ†æï¼‰
USER_PROFILE = """
## ç”¨æˆ¶èƒŒæ™¯
- èº«ä»½ï¼šå¿ƒè‡Ÿå¤–ç§‘é†«å¸«
- å°ˆæ¥­é ˜åŸŸï¼šECMOã€VADã€ç“£è†œæ‰‹è¡“ï¼ˆMorrow techniqueã€AVRï¼‰ã€è¡“å¾Œé‡ç—‡ç…§è­·

## èˆˆè¶£åå¥½
- é†«å­¸ï¼šå¿ƒè‡Ÿå¤–ç§‘æ–°æŠ€è¡“ã€ECMO/VAD ç®¡ç†ã€è¡“å¾Œä½µç™¼ç—‡è™•ç†ã€è—¥ç‰©ä½¿ç”¨ï¼ˆVancomycinã€Meropenemã€Milrinoneï¼‰
- AI/æŠ€è¡“ï¼šClaude Codeã€LLM æ‡‰ç”¨ã€è‡ªå‹•åŒ–å·¥ä½œæµï¼ˆn8nï¼‰ã€AI Agent
- çŸ¥è­˜ç®¡ç†ï¼šZettelkastenã€Heptabaseã€ç¬¬äºŒå¤§è…¦ã€ç­†è¨˜æ–¹æ³•è«–
- æ€è€ƒï¼šå–œæ­¡æ·±åº¦åˆ†æå’Œå¯¦ç”¨æ¡ˆä¾‹ï¼Œåå¥½æœ‰æ´è¦‹çš„å…§å®¹è€Œéç´”æ–°è

## ç¯©é¸åŸå‰‡
- å„ªå…ˆï¼šæœ‰å¯¦éš›æ‡‰ç”¨åƒ¹å€¼ã€æŠ€è¡“æ·±åº¦ã€ç¨ç‰¹è§€é»çš„æ–‡ç« 
- æ¬¡è¦ï¼šç”¢æ¥­è¶¨å‹¢ã€å·¥å…·æ›´æ–°
- é¿å…ï¼šç´”ç²¹çš„æ–°èé€Ÿå ±ã€æ¨™é¡Œé»¨ã€é‡è¤‡å…§å®¹
"""


def ai_filter_articles(articles: List[Dict], domain: str, max_items: int) -> List[Dict]:
    """
    ä½¿ç”¨ AI ç¯©é¸æ–‡ç« ä¸¦ç”¢ç”Ÿæ‘˜è¦ï¼ˆæ ¹æ“šç”¨æˆ¶åå¥½ï¼‰
    """
    if not articles:
        return []

    import anthropic
    import json

    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

    # æº–å‚™æ–‡ç« åˆ—è¡¨ï¼ˆåŒ…å«æ‘˜è¦ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
    articles_text = []
    for i, article in enumerate(articles[:20]):
        summary = article.get('summary', '')[:100] if article.get('summary') else ''
        articles_text.append(f"{i+1}. [{article.get('source')}] {article.get('title')}\n   æ‘˜è¦: {summary}")

    domain_context = {
        "medical": "ECMOã€VADã€å¿ƒè‡Ÿå¤–ç§‘ã€é‡ç—‡é†«å­¸ç›¸é—œ",
        "ai": "AIã€LLMã€Claudeã€æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ç›¸é—œ",
        "international": "åœ‹éš›æƒ…å‹¢ã€åœ°ç·£æ”¿æ²»ã€å…¨çƒäº‹å‹™ç›¸é—œ",
        "github": "GitHub é–‹æºå°ˆæ¡ˆã€ç¨‹å¼é–‹ç™¼ã€æŠ€è¡“å·¥å…·ç›¸é—œ",
        "knowledge": "çŸ¥è­˜ç®¡ç†ã€ç”Ÿç”¢åŠ›ã€å­¸ç¿’æ–¹æ³•ã€ç­†è¨˜å·¥å…·ç›¸é—œ",
        "claude-code": "Claude Code ç‰ˆæœ¬æ›´æ–°ã€æ–°åŠŸèƒ½ã€bug ä¿®å¾©"
    }

    prompt = f"""ä½ æ˜¯å€‹äººåŒ–è³‡è¨Šç¯©é¸åŠ©æ‰‹ã€‚æ ¹æ“šç”¨æˆ¶çš„èƒŒæ™¯å’Œåå¥½ï¼Œå¾æ–‡ç« ä¸­é¸å‡ºæœ€ç¬¦åˆä»–èˆˆè¶£çš„å…§å®¹ã€‚

{USER_PROFILE}

## æœ¬æ¬¡ä»»å‹™
é ˜åŸŸï¼š{domain_context.get(domain, '')}
å¾ä»¥ä¸‹æ–‡ç« ä¸­é¸å‡ºæœ€ç¬¦åˆç”¨æˆ¶èˆˆè¶£çš„ {max_items} ç¯‡ï¼š

{chr(10).join(articles_text)}

è«‹ç”¨ JSON æ ¼å¼å›è¦†ï¼ŒåŒ…å«ï¼š
1. é¸ä¸­çš„æ–‡ç« ç·¨è™Ÿ
2. æ¯ç¯‡æ–‡ç« çš„ä¸€å¥è©±é‡é»ï¼ˆèªªæ˜ç‚ºä»€éº¼é€™ç¯‡å°ã€Œé€™ä½ç”¨æˆ¶ã€ç‰¹åˆ¥æœ‰åƒ¹å€¼ï¼‰

æ ¼å¼ç¯„ä¾‹ï¼š
{{"selected": [1, 3, 5], "highlights": {{"1": "é¦–å€‹...", "3": "çªç ´...", "5": "æœ€æ–°..."}}}}

åªå›è¦† JSONï¼Œä¸è¦å…¶ä»–èªªæ˜ã€‚"""

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}]
        )

        response = message.content[0].text.strip()

        # ç§»é™¤ markdown ç¨‹å¼ç¢¼å€å¡ŠåŒ…è£
        if response.startswith("```"):
            lines = response.split("\n")
            # ç§»é™¤é¦–è¡Œ ```json å’Œå°¾è¡Œ ```
            if lines[0].startswith("```"):
                lines = lines[1:]
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            response = "\n".join(lines)

        # å˜—è©¦è§£æ JSON
        try:
            data = json.loads(response)
            selected_indices = [int(x) - 1 for x in data.get("selected", [])]
            highlights = data.get("highlights", {})

            filtered = []
            for i in selected_indices:
                if 0 <= i < len(articles):
                    article = articles[i].copy()
                    # åŠ å…¥ AI ç”Ÿæˆçš„é‡é»æ‘˜è¦
                    article["highlight"] = highlights.get(str(i + 1), "")
                    filtered.append(article)

            return filtered[:max_items]
        except json.JSONDecodeError:
            # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œå˜—è©¦èˆŠæ ¼å¼ï¼ˆç´”ç·¨è™Ÿï¼‰
            selected_indices = [int(x.strip()) - 1 for x in response.split(",") if x.strip().isdigit()]
            filtered = [articles[i] for i in selected_indices if 0 <= i < len(articles)]
            return filtered[:max_items]

    except Exception as e:
        print(f"  AI filter error: {e}")
        return articles[:max_items]


def format_domain_message(articles: List[Dict], domain: str, date_str: str) -> str:
    """
    æ ¼å¼åŒ–é ˜åŸŸæ¨æ’­è¨Šæ¯
    """
    config = DOMAIN_CONFIG.get(domain, {})
    emoji = config.get("emoji", "ğŸ“°")
    name = config.get("name", domain)

    if not articles:
        return f"{emoji} <b>{name} - {date_str}</b>\n\nç›®å‰æ²’æœ‰æ–°å…§å®¹ã€‚"

    lines = [f"{emoji} <b>{name} - {date_str}</b>ï¼ˆ{len(articles)} ç¯‡ï¼‰\n"]

    for article in articles:
        title = article.get("title", "ç„¡æ¨™é¡Œ")
        if len(title) > 60:
            title = title[:57] + "..."

        source = article.get("source", "")
        link = article.get("link", "")
        highlight = article.get("highlight", "")

        lines.append(f"â€¢ <b>{title}</b>")
        if highlight:
            lines.append(f"  ğŸ’¡ {highlight}")
        if source:
            lines.append(f"  ğŸ“ {source}")
        if link:
            lines.append(f"  ğŸ”— <a href=\"{link}\">é–±è®€</a>")
        lines.append("")

    return "\n".join(lines)


def send_telegram_message(text: str) -> bool:
    """ç™¼é€ Telegram è¨Šæ¯"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }

    response = requests.post(url, json=payload)
    return response.status_code == 200


def run_domain_digest(domain: str, hours: int = None, dry_run: bool = False):
    """
    åŸ·è¡Œç‰¹å®šé ˜åŸŸçš„æ¨æ’­

    Args:
        domain: é ˜åŸŸåç¨±
        hours: ç²å–éå»å¹¾å°æ™‚çš„æ–‡ç« ï¼ˆè‹¥æœªæŒ‡å®šå‰‡ä½¿ç”¨é ˜åŸŸé è¨­å€¼ï¼‰
        dry_run: æ¸¬è©¦æ¨¡å¼
    """
    config = DOMAIN_CONFIG.get(domain)
    if not config:
        print(f"Unknown domain: {domain}")
        print(f"Available domains: {', '.join(DOMAIN_CONFIG.keys())}")
        return False

    # ä½¿ç”¨é ˜åŸŸé è¨­æ™‚é–“çª—å£æˆ–é è¨­ 24 å°æ™‚
    if hours is None:
        hours = config.get("default_hours", 24)

    print("=" * 60)
    print(f"é ˜åŸŸæ¨æ’­ï¼š{config['emoji']} {config['name']}")
    print(f"æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"æ¨¡å¼ï¼š{'æ¸¬è©¦' if dry_run else 'æ­£å¼'}ï¼Œæ™‚é–“çª—å£ï¼š{hours}h")
    print("=" * 60)

    # 1. ç²å–æ–‡ç« 
    print(f"\n[1/3] ç²å–æ–‡ç« ...")
    articles = fetch_domain_articles(domain, hours)
    print(f"  å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

    if not articles:
        print("  æ²’æœ‰æ–°æ–‡ç« ")
        if not dry_run:
            send_telegram_message(f"{config['emoji']} <b>{config['name']}</b>\n\néå» {hours} å°æ™‚æ²’æœ‰æ–°å…§å®¹ã€‚")
        return True

    # 2. ç¯©é¸ä¸¦ç”¢ç”Ÿæ‘˜è¦
    print(f"\n[2/3] ç¯©é¸æ–‡ç« ...")
    if config.get("use_ai_filter"):
        print("  ä½¿ç”¨ AI ç¯©é¸...")
        filtered = ai_filter_articles(articles, domain, config["max_items"])
    else:
        filtered = articles[:config["max_items"]]
    print(f"  ç²¾é¸ {len(filtered)} ç¯‡")

    # 3. æ¨æ’­
    print(f"\n[3/3] æ¨æ’­...")
    date_str = datetime.now().strftime("%Y-%m-%d")
    message = format_domain_message(filtered, domain, date_str)

    if dry_run:
        print("  (æ¸¬è©¦æ¨¡å¼) è¨Šæ¯å…§å®¹ï¼š")
        print("-" * 40)
        # ç§»é™¤ HTML tags é¡¯ç¤º
        import re
        clean_msg = re.sub(r'<[^>]+>', '', message)
        print(clean_msg)
        print("-" * 40)
    else:
        success = send_telegram_message(message)
        print(f"  {'âœ“ æ¨æ’­æˆåŠŸ' if success else 'âœ— æ¨æ’­å¤±æ•—'}")

    print("\n" + "=" * 60)
    print("å®Œæˆ")
    print("=" * 60)

    return True


def run_all_domains(dry_run: bool = False):
    """åŸ·è¡Œæ‰€æœ‰é ˜åŸŸçš„æ¨æ’­"""
    for domain in DOMAIN_CONFIG.keys():
        run_domain_digest(domain, dry_run=dry_run)
        print("\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åˆ†é ˜åŸŸæ¨æ’­ç³»çµ±")
    parser.add_argument("domain", nargs="?", default="all",
                       help="é ˜åŸŸåç¨± (medical, ai, international, github, knowledge, all)")
    parser.add_argument("--hours", type=int, default=None,
                       help="ç²å–éå»å¹¾å°æ™‚çš„æ–‡ç«  (é è¨­ä¾é ˜åŸŸè¨­å®š)")
    parser.add_argument("--dry-run", action="store_true",
                       help="æ¸¬è©¦æ¨¡å¼ï¼Œä¸å¯¦éš›ç™¼é€")
    parser.add_argument("--list", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨é ˜åŸŸ")

    args = parser.parse_args()

    if args.list:
        print("å¯ç”¨é ˜åŸŸï¼š")
        for key, config in DOMAIN_CONFIG.items():
            print(f"  {config['emoji']} {key}: {config['name']}")
        sys.exit(0)

    # é©—è­‰é…ç½®
    try:
        validate_config()
    except ValueError as e:
        print(f"é…ç½®éŒ¯èª¤: {e}")
        sys.exit(1)

    if args.domain == "all":
        run_all_domains(dry_run=args.dry_run)
    else:
        run_domain_digest(args.domain, hours=args.hours, dry_run=args.dry_run)
